\documentclass[11pt,a4paper]{article}

% ── Packages ──────────────────────────────────────────────
\usepackage[margin=2.5cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{float}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    citecolor=blue
}

% ── Placeholder command ───────────────────────────────────
\newcommand{\todo}[1]{{\color{red}\textbf{[TODO: #1]}}}

% ── Title ─────────────────────────────────────────────────
\title{%
    \textbf{Natural Language Processing Coursework} \\[0.3em]
    \large COMP60035
}
\author{
    Adam Kassam \\
    CID: 02374184 \\ [1em]
    Leaderboard Name: AK \\[0.5em]
    Repository: \url{https://github.com/aak523/nlp-cw}
}
\date{\today}

\begin{document}
\maketitle

% ==============================================================
\section{Critical Paper Review}
\label{sec:ex1}
% [6 Marks | up to 3 Hours]
% ==============================================================

\subsection*{Q1. Primary Contributions}

The authors address the problem of detecting patronising and condescending language (PCL) towards vulnerable communities in news media. PCL is a subtle, often well-intentioned form of harmful language that had been largely overlooked by NLP research in favour of more explicit phenomena such as hate speech and offensive language.

The authors' primary contribution is a new annotated dataset of over 10,000 paragraphs extracted from the News on Web (NoW) corpus, retrieved using ten keywords related to vulnerable groups (e.g.\ \textit{homeless}, \textit{refugee}, \textit{immigrant}) across 20 English-speaking countries.

Alongside the dataset, they propose a two-level taxonomy of PCL categories: three high-level groups (\textit{The Saviour}, \textit{The Expert}, \textit{The Poet}) subdivided into seven fine-grained subcategories such as \textit{Unbalanced power relations}, \textit{Shallow solution}, and \textit{Metaphor}.

The paper also provides exploratory baselines using SVMs, a BiLSTM, and fine-tuned language models (BERT variants and RoBERTa) across two tasks: binary PCL detection and multi-label PCL categorisation.

\subsection*{Q2. Technical Strengths}

\paragraph{Originality and significance.} The dataset fills a genuine gap in NLP research on subtle harmful language. While prior work such as Wang and Potts (2019) modelled condescension in direct communication, this paper targets PCL in news media directed at vulnerable communities, which is a distinct and understudied setting. The two-level taxonomy provides a principled framework for categorising PCL that goes beyond binary detection.

\paragraph{Annotation methodology.} The annotation process is thorough: two primary annotators labelled the full dataset on a three-level scale, with a third annotator resolving total disagreements. Individual labels were combined into a five-point scale capturing degrees of agreement, and a second annotation pass used the BRAT tool to mark specific PCL spans with subcategory labels.

\paragraph{Evaluation design.} The decomposition into binary detection (Task~1) and multi-label categorisation (Task~2) is well-motivated and provides a broad view of model capabilities. The choice of precision, recall, and F1 for the positive class is appropriate given the severe class imbalance (only 995 of 10,637 paragraphs are positive), where accuracy alone would be misleading. The error analysis is one of the paper's strongest elements: concrete misclassification examples are provided with specific explanations, revealing that false positives arise from superficially PCL-like language while false negatives involve categories requiring world knowledge.

\paragraph{Accessibility.} The paper is clearly written. The sociolinguistics background and taxonomy definitions are accessible without prior NLP expertise.

\subsection*{Q3. Key Weaknesses}

\paragraph{Limited modelling contribution.} The authors apply well-known baselines (SVMs, BiLSTM, BERT variants) without proposing any novel architecture or training strategy tailored to PCL. They identify that certain categories (e.g.\ \textit{Metaphor}) require world knowledge, yet do not explore methods to inject such knowledge (e.g.\ knowledge graph embeddings or auxiliary training objectives). The choice to represent paragraphs using averaged Word2Vec embeddings for the SVM-WV baseline, which entirely discards word order, is also not discussed, despite this seeming particularly problematic for detecting subtle language patterns.

\paragraph{Absence of ablation studies.} The paper includes no ablations. For example, it does not investigate whether the retrieval keyword affects model performance, whether the annotation threshold (labels $\geq 2$ as positive vs.\ other cutoffs) impacts results, or whether providing the keyword as an additional input feature helps. This makes it difficult to understand \textit{why} the models succeed or fail in specific cases.

\paragraph{Reproducibility gaps.} While hyperparameters for SVMs and the BiLSTM are reported and the dataset is released, several details needed for exact reproduction are missing: learning rates for BERT fine-tuning are not stated, the specific cross-validation fold splits are neither provided nor described, and preprocessing steps (e.g.\ tokenisation procedures per model) are not detailed. Approximate reproduction should be feasible, but exact replication would require guesswork.

% ==============================================================
\section{Exploratory Data Analysis}
\label{sec:ex2}
% [6 Marks | up to 3 Hours]
% Two distinct EDA techniques, 3 marks each.
% ==============================================================

\subsection{Class Imbalance \& Sequence Length Profiling}

\paragraph{Visual/Tabular Evidence.} See Figure~\ref{fig:eda1} and Table~\ref{tab:eda1-wordcount}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/eda1_class_and_length.png}
    \caption{(a) Binary class distribution showing severe imbalance (90.5\% negative vs.\ 9.5\% positive). (b) Fine-grained annotator label distribution (0--4), with the majority of positive examples receiving label 2 (borderline agreement). (c) Word count box plots by label, showing that PCL-positive paragraphs tend to be longer (median 47 vs.\ 42 words).}
    \label{fig:eda1}
\end{figure}

% Example table placeholder:
\begin{table}
  \centering
  \caption{Word count summary statistics by binary    
label.}
  \label{tab:eda1-wordcount}
  \begin{tabular}{lrrrrrr}
      \toprule
      \textbf{Label} & \textbf{N} & \textbf{Mean} &   
\textbf{Std} & \textbf{Min} & \textbf{Median} &
\textbf{Max} \\
      \midrule
      Not PCL (0) & 9473 & 47.9 & 28.6 & 0 & 42 & 909 \\
      PCL (1)     &  993 & 53.6 & 32.8 & 6 & 47 & 512 \\
      \bottomrule
  \end{tabular}
  \par\smallskip
  \small Overall positive rate: 9.5\% (993/10{,}466). 
Imbalance ratio (neg:pos): 9.5:1.
\end{table}

\paragraph{Analysis.}

The dataset exhibits \textbf{severe class imbalance}: only around 9.5\% of paragraphs are labelled as PCL (positive), yielding a negative-to-positive ratio of roughly 9.5:1. The fine-grained label distribution (panel b) reveals that the majority of positives come from label 2 (borderline agreement between annotators), with very few paragraphs receiving the strongest label of 4. This suggests a long tail of "soft" positives where annotators only partially agreed on the presence of PCL.

Panel (c) shows that \textbf{PCL-positive paragraphs tend to be longer} than negative ones. The median word count for positive examples is notably higher, and the interquartile range is wider. This makes intuitive sense: patronising language often involves elaboration, storytelling, or emotive description, all of which produce longer text.

\paragraph{Impact Statement.}

\begin{enumerate}
    \item \textbf{Class-weighted training.} A naive classifier predicting "Not PCL" for every input would achieve \~90.5\% accuracy, so accuracy is a misleading metric. We will use \textbf{F1 for the positive class} as the primary metric (matching the shared task) and address the imbalance during training via class-weighted cross-entropy loss or focal loss.
    \item \textbf{Tokeniser \texttt{max\_length}.} The word-count distribution informs our choice of maximum sequence length for the transformer tokeniser. Since PCL paragraphs skew longer and we do not want to truncate informative content, we should set \texttt{max\_length} generously (e.g. 256 subword tokens) and verify that truncation affects only a negligible fraction of positive examples.
\end{enumerate}

\subsection{Discriminative $N$-gram Analysis}

\paragraph{Visual/Tabular Evidence.}

See Figure~\ref{fig:eda2}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/eda2_ngram_log_odds.png}
    \caption{Log-odds ratio of bigram frequencies       
between PCL-positive and PCL-negative classes. Left:
bigrams most associated with patronising language.
Right: bigrams most associated with non-patronising     
language. Higher absolute values indicate stronger class
association.}
    \label{fig:eda2}
\end{figure}

\paragraph{Analysis.}

We compute the \textbf{log-odds ratio} of bigram frequencies between the PCL-positive and PCL-negative classes. This measures how much more (or less) likely each bigram is to appear in patronising text versus non-patronising text, after Laplace smoothing.

The PCL-associated bigrams fall into clear \textbf{patronising archetypes} from the paper's taxonomy:
\begin{itemize}
    \item \textbf{Saviour framing:} \textit{"feed hungry"}, \textit{"families help"}, \textit{"need giving"}, \textit{"giving need"}, \textit{"reaching need"}, \textit{"restore hope"} --- language that positions the speaker as a rescuer.
    \item \textbf{Compassion / pity:} \textit{"feel sorry"}, \textit{"smiles faces"}, \textit{"hope hopeless"} --- emotive, flowery descriptions that sentimentalise hardship.
    \item \textbf{Power asymmetry:} \textit{"burden society"}, \textit{"poverty stricken"}, \textit{"think poor"}, \textit{"little girl"} --- language that reduces people to their vulnerability.
    \item \textbf{Shallow solutions / clichés:} \textit{"mother teresa"}, \textit{"developed country"} --- invoking well-known figures or broad generalisations as substitutes for substantive discussion.
\end{itemize}

The Not-PCL bigrams are strikingly different in character: they are \textbf{factual, institutional, or geographic}: \textit{"said statement"}, \textit{"humanitarian assistance"}, \textit{"refugee camp"}, \textit{"social security"}, \textit{"new zealand"}, \textit{"los angeles"}, \textit{"50 000"}. Several relate to neutral reporting (\textit{"anti immigrant"}, \textit{"illegal immigrants"}, \textit{"afghan refugees"}) or clinical descriptions (\textit{"disabled list"}, \textit{"intellectually disabled"}). These bigrams describe vulnerable groups without the emotive or condescending framing that characterises PCL.

This confirms the PCL paper's finding that certain categories (e.g. \textbf{Compassion}, \textbf{Unbalanced power relations}) rely on identifiable lexical cues, while also showing that the boundary between PCL and neutral text is \textbf{contextual}. Words like \textit{"need"}, \textit{"hope"}, and \textit{"poor"} appear in both classes, and their patronising interpretation depends on surrounding framing.

\paragraph{Impact Statement.}

\begin{enumerate}
    \item \textbf{Contexualised model required.} Individual words like \textit{"need"}, \textit{"hope"}, and \textit{"poor"} appear in both classes; it is the surrounding framing that makes them patronising. A bag-of-words or averaged-embedding approach would conflate these usages, confirming that a \textbf{contextualised language model} (e.g. RoBERTa) is necessary to capture the distinction.
    \item \textbf{Minimal text preprocessing.} The discriminative bigrams consist of common English words in specific combinations. Aggressive preprocessing (e.g. stop-word removal or lemmatisation) would destroy phrases like \textit{"feel sorry"} or \textit{"need giving"} that carry the patronising signal. We should feed \textbf{raw text} into the transformer tokeniser with no cleaning beyond what the tokeniser handles natively.
\end{enumerate}

% ==============================================================
\section{Proposed Approach}
\label{sec:ex3}
% [4 Marks | up to 2 Hours]
% ==============================================================

\subsection{Model Selection}

We fine-tune \textbf{DeBERTa-V3}~\cite{he2021debertav3} for binary sequence classification. We evaluate two variants: \texttt{microsoft/deberta-v3-base} (86M parameters) and \texttt{microsoft/deberta-v3-large} (304M parameters), selecting the final configuration by development-set F1.

DeBERTa-V3 improves on the RoBERTa-base~\cite{liu2019roberta} baseline (F1\,=\,0.48 dev, 0.49 test) in two important respects. First, it uses \textbf{disentangled attention}~\cite{he2021deberta}, which encodes token content and relative position via separate matrices and computes attention scores over both simultaneously. This is particularly well-suited to PCL detection, where condescension is expressed through phrasing and word order rather than individual lexical items alone. Second, the V3 series replaces masked language modelling with an \textbf{ELECTRA-style replaced-token-detection} pre-training objective~\cite{clark2020electra} on a significantly larger corpus, yielding richer contextual representations. Despite being ``base''-sized, \texttt{deberta-v3-base} surpasses RoBERTa-large on standard benchmarks, making it a well-motivated upgrade.

\subsection{Training Methodology}

The model is fine-tuned end-to-end with the following design decisions, each motivated by the dataset characteristics identified in Section~\ref{sec:ex2}.

\paragraph{Class-weighted cross-entropy loss.} The 9.5:1 negative-to-positive imbalance means a trivial classifier would achieve 90.5\% accuracy. We assign balanced class weights $w_c = N_{\text{total}} / (2 N_c)$ to the cross-entropy loss, increasing the gradient contribution of each positive example in proportion to its under-representation.

\paragraph{Label smoothing.} We apply a smoothing coefficient of $\varepsilon = 0.1$. With only ${\sim}500$ positive training examples, many of which are borderline positives where annotators only partially agreed (Figure~\ref{fig:eda1}b), the model can easily become overconfident, causing erratic precision--recall swings across epochs. Label smoothing encourages calibrated probability estimates rather than collapsed outputs near 0 or 1.

\paragraph{Differential learning rates.} The classifier head is updated at a higher rate ($5 \times 10^{-5}$) than the backbone ($8 \times 10^{-6}$). The backbone weights are already near-optimal from pre-training; large updates risk destroying the representations that give DeBERTa-V3 its advantage.

\paragraph{Threshold tuning.}
Rather than classifying at the default threshold of 0.5, we sweep thresholds over $[0.30, 0.60]$ on the development set and select the value maximising F1 for the positive class. Given the severe class imbalance, the optimal threshold is typically below 0.5.

\paragraph{Regularisation and early stopping.}
We apply weight decay of 0.1, classifier dropout of 0.3, and gradient clipping (max norm 1.0). Early stopping with patience 5 selects the checkpoint with the best development F1 across up to 15 epochs.

\subsection*{Rationale and Expected Outcome}

Two findings from the EDA directly motivate this approach. First, the boundary between PCL and neutral language is highly contextual (Section~\ref{sec:ex2}): words such as \textit{``need''}, \textit{``hope''}, and \textit{``poor''} appear in both classes, and it is the surrounding vocabulary that signals condescension. This rules out bag-of-words or
averaged-embedding methods and confirms the need for a contextualised transformer. Second, positive examples tend to be longer than negative ones, motivating \texttt{max\_length\,=\,256} to avoid truncating discriminative content.

DeBERTa-V3's disentangled attention is well-suited to capturing the patterns identified in the bigram analysis (Figure~\ref{fig:eda2}). For example, saviour framing (\textit{``feed hungry''}, \textit{``restore hope''}) and pity language (\textit{``feel sorry''}) that depend not only on the words themselves but on the surrounding structure. The paper's own error analysis (Section~\ref{sec:ex1}) showed that its BERT-based models struggled with cases requiring world knowledge and contextual framing; the V3 pre-training objective, which conditions every token on its full context, provides stronger representations for exactly this type of inference.

We expect \texttt{deberta-v3-base} to comfortably exceed the RoBERTa-base baseline. Upgrading to \texttt{deberta-v3-large} may yield a further improvement at the cost of additional training time, and we report results for the better-performing configuration.

% ==============================================================
\section{Implementation and Global Evaluation}
\label{sec:ex4-51}
% Exercise 4: [1 Mark] -- repository link
% Exercise 5.1: [6 Marks] -- dev.txt and test.txt
% ==============================================================

The code, trained model, and prediction files (\texttt{dev.txt},
\texttt{test.txt}) are available in the \texttt{BestModel/} folder of the
repository:

\begin{center}
    \url{https://github.com/aak523/nlp-cw}
\end{center}

% ==============================================================
\section{Local Evaluation}
\label{sec:ex52}
% [5 Marks | up to 3 Hours]
% Error Analysis (2.5 marks) + Other local evaluation (2.5 marks)
% ==============================================================

\subsection{Error Analysis}

\todo{Manually inspect samples where the model failed on the official dev
set. Categorise the types of errors. Are there patterns -- e.g.\ sarcasm,
specific keywords, sentence length? Include concrete examples in a table.}

% Example error analysis table:
% \begin{table}
%     \centering
%     \caption{Examples of misclassified instances on the dev set.}
%     \label{tab:errors}
%     \begin{tabular}{p{0.55\textwidth}ccc}
%         \toprule
%         \textbf{Text (truncated)} & \textbf{True} & \textbf{Pred} & \textbf{Error Type} \\
%         \midrule
%         \todo{Example 1...} & 1 & 0 & \todo{Type} \\
%         \todo{Example 2...} & 0 & 1 & \todo{Type} \\
%         \bottomrule
%     \end{tabular}
% \end{table}

\todo{Discuss what these errors reveal about the model's behaviour and
limitations.}

\subsection{Additional Local Evaluation}

\todo{Perform at least one additional local evaluation technique. Options
include: ablation study, confusion matrix analysis, precision--recall curves,
comparison with baseline predictions, per-category performance breakdown,
or custom metrics. Include figures/tables and explain what they reveal.}

% Example confusion matrix placeholder:
% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\textwidth]{figures/confusion_matrix.png}
%     \caption{\todo{Confusion matrix on the official dev set.}}
%     \label{fig:cm}
% \end{figure}

% ==============================================================
% References
% ==============================================================
\bibliographystyle{plain}
\bibliography{references}

\end{document}